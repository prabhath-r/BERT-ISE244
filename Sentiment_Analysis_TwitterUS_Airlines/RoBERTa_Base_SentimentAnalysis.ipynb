{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part-1 **Sentiment Analysis using BERT on Twitter US-Airlines Sentiment dataset **"
      ],
      "metadata": {
        "id": "tAMB6A4nXg0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa-base**"
      ],
      "metadata": {
        "id": "Ni7CMxLtXoYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DjwiP-11oj_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f85f8e2-827d-488e-9264-a197dc269ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# !pip install transformers \n",
        "# !pip install sentencepiece\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import RobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Tweets.csv')"
      ],
      "metadata": {
        "id": "iv5PvHlNyEyJ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHmYeGm2qYK2",
        "outputId": "f2ec6fe9-794b-48e4-87fc-bddc304d5d34"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install vaderSentiment\n",
        "import vaderSentiment"
      ],
      "metadata": {
        "id": "GFMVxjJzQ2bU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing functions\n",
        "def remove_usernames(text):\n",
        "    return re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    # Remove URLs\n",
        "    tweet = remove_urls(tweet)\n",
        "    \n",
        "    # Tokenize the tweet\n",
        "    tokens = word_tokenize(tweet)\n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    # Join the tokens into a string\n",
        "    tweet = ' '.join(tokens)\n",
        "    \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "pHeBreS7rRAs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the 'text' column\n",
        "df['text'] = df['text'].apply(remove_usernames)\n",
        "df['text'] = df['text'].apply(preprocess_tweet_text)"
      ],
      "metadata": {
        "id": "VKaa3x08DnVE"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['airline_sentiment'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['airline_sentiment'])\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "# Load pre-trained DistilBERT tokenizer and encode text\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True)\n",
        "\n",
        "df['airline_sentiment'] = df['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
        "                              torch.tensor(train_encodings['attention_mask']),\n",
        "                              torch.tensor(train_labels))\n",
        "\n",
        "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
        "                            torch.tensor(val_encodings['attention_mask']),\n",
        "                            torch.tensor(val_labels))\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
        "                             torch.tensor(test_encodings['attention_mask']),\n",
        "                             torch.tensor(test_labels))"
      ],
      "metadata": {
        "id": "ub585LnuxbWx"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorizing the data using data loaders \n",
        "def get_data_loaders(train_inputs, train_labels, val_inputs, val_labels, batch_size):\n",
        "    # Convert data to PyTorch tensors\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_inputs = torch.tensor(val_inputs)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create TensorDataset objects\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    \n",
        "    # Create DataLoader objects\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "ud4lEcku5JmJ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "# Define data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load pre-trained roberta-base model\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 3)\n",
        "\n",
        "# Move model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "num_warmup_steps = int(len(train_dataloader) * 0.1)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=5)\n",
        "epochs = 5\n",
        "\n",
        "# Define cross-entropy loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define early_stop\n",
        "early_stop = 3\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_acc += (outputs[1].detach().cpu().numpy().argmax(axis=1) == b_labels.cpu().numpy()).mean()\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_acc /= len(train_dataloader)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (outputs[1].detach().cpu().numpy().argmax(axis=1) == b_labels.cpu().numpy()).mean()\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_acc /= len(val_dataloader)\n",
        "\n",
        "    print(\"Epoch {} - train loss: {:.3f} - train acc: {:.3f} - val loss: {:.3f} - val acc: {:.3f}\".format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "    # Save the model\n",
        "    if val_loss < best_val_loss:\n",
        "        torch.save(model.state_dict(), 'roberta_sentiment_model.pt')\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        print(\"The model has been saved\")\n",
        "\n",
        "    # Stop training if the validation loss stops improving after certain epochs\n",
        "    if epoch - best_epoch >= early_stop:\n",
        "        print(\"Validation loss has not improved in {} epochs, stopping training\".format(early_stop))\n",
        "        break"
      ],
      "metadata": {
        "id": "don7gd9S78Fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f950bb-52e1-43e0-b2ca-ddb21220bc38"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - train loss: 0.788 - train acc: 0.668 - val loss: 0.742 - val acc: 0.685\n",
            "The model has been saved\n",
            "Epoch 1 - train loss: 0.771 - train acc: 0.674 - val loss: 0.742 - val acc: 0.685\n",
            "Epoch 2 - train loss: 0.772 - train acc: 0.674 - val loss: 0.742 - val acc: 0.685\n",
            "Epoch 3 - train loss: 0.772 - train acc: 0.672 - val loss: 0.742 - val acc: 0.685\n",
            "Validation loss has not improved in 3 epochs, stopping training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch size 8** taking about **200 seconds** for each episode to run.\n",
        "\n",
        "Epoch 0 - train loss: 0.788 - train acc: 0.668 - val loss: 0.742 - val acc: 0.685\n",
        "\n",
        "Epoch 1 - train loss: 0.771 - train acc: 0.674 - val loss: 0.742 - val acc: 0.685\n",
        "\n",
        "Epoch 2 - train loss: 0.772 - train acc: 0.674 - val loss: 0.742 - val acc: 0.685\n",
        "\n",
        "Epoch 3 - train loss: 0.772 - train acc: 0.672 - val loss: 0.742 - val acc: 0.685\n",
        "\n",
        "**Batch size 16** taking about **200 seconds** for each episode to run.\n",
        "\n",
        "Epoch 0 - train loss: 0.837 - train acc: 0.606 - val loss: 0.796 - val acc: 0.629\n",
        "\n",
        "Epoch 1 - train loss: 0.817 - train acc: 0.627 - val loss: 0.796 - val acc: 0.629\n",
        "\n",
        "Epoch 2 - train loss: 0.817 - train acc: 0.627 - val loss: 0.796 - val acc: 0.629\n",
        "\n",
        "Epoch 3 - train loss: 0.816 - train acc: 0.627 - val loss: 0.796 - val acc: 0.629\n",
        "\n",
        "**Batch size 32** taking about **150 seconds** for each episode to run.\n",
        "\n",
        "Epoch 0 - train loss: 0.914 - train acc: 0.618 - val loss: 0.890 - val acc: 0.628\n",
        "\n",
        "Epoch 1 - train loss: 0.900 - train acc: 0.627 - val loss: 0.890 - val acc: 0.628\n",
        "\n",
        "Epoch 2 - train loss: 0.897 - train acc: 0.627 - val loss: 0.890 - val acc: 0.628\n",
        "\n",
        "Epoch 3 - train loss: 0.899 - train acc: 0.627 - val loss: 0.890 - val acc: 0.628\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iwp4XDOuAbpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from the Roberta(base) model:**\n",
        "\n",
        "The Roberta model achieves a training accuracy of 67.4% and a validation accuracy of 68.5%, with a loss of 0.788 after the first epoch. The model improves slightly in the second epoch but does not improve further in the subsequent epochs. The validation loss remains constant at 0.742 from the second epoch onwards. This suggests that the model might have reached the highest it can learn, based on the parameters for the data provided.\n",
        "\n",
        "It is interesting and worth noting that the Roberta (base) model performs better than the (base) DistilBERT model in terms of accuracy, as the former achieves a higher validation accuracy. However, the Roberta model does not improve significantly in terms of accuracy after the first epoch, which suggests that the model might not be able to learn much from the data after the initial training. This could be attributed to factors such as the size and quality of the training data, as well as the complexity of the model architecture."
      ],
      "metadata": {
        "id": "Zr8o3fySW5M-"
      }
    }
  ]
}